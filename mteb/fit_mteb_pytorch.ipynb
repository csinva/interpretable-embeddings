{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "loading qa embeddings...\n",
      "computing tf-idf embeddings...\n",
      "data has 4000 queries and 5210 documents\n",
      "TF-IDF Test: mrr=0.79\\pm0.01, top1_frac=0.71\\pm0.01,  top3_frac=0.86\\pm0.01\n",
      "QA Test: mrr=0.45\\pm0.01, top1_frac=0.34\\pm0.01,  top3_frac=0.50\\pm0.01\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from os.path import join, expanduser\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.optim\n",
    "from fit_mteb_pytorch import *\n",
    "\n",
    "# Load embeddings for corpus and queries\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "dset = MiniMarcoDataset()\n",
    "dset.query_ids = dset.query_ids[:4000]\n",
    "print('data has', len(dset), 'queries and', len(dset.corpus_ids), 'documents')\n",
    "query_ids_train, query_ids_test = train_test_split(\n",
    "    dset.query_ids, random_state=1, test_size=0.75)\n",
    "\n",
    "# evaluate base embeddings\n",
    "embs_qa, embs_tfidf, labels = dset[query_ids_test]\n",
    "mrr, top1_frac, top3_frac, mrr_sem, top1_frac_sem, top3_frac_sem = evaluate_retrieval(\n",
    "    embs_tfidf, dset.embs_tfidf_corpus_df.values,\n",
    "    labels=labels, corpus_ids=dset.corpus_ids)\n",
    "print(f'TF-IDF Test: {mrr=:.2f}\\pm{mrr_sem:0.2f}, {top1_frac=:.2f}\\pm{top1_frac_sem:0.2f},  {top3_frac=:.2f}\\pm{top3_frac_sem:0.2f}')\n",
    "mrr, top1_frac, top3_frac, mrr_sem, top1_frac_sem, top3_frac_sem = evaluate_retrieval(\n",
    "    embs_qa, dset.embs_qa_corpus_df.values,\n",
    "    labels=labels, corpus_ids=dset.corpus_ids)\n",
    "print(f'QA Test: {mrr=:.2f}\\pm{mrr_sem:0.2f}, {top1_frac=:.2f}\\pm{top1_frac_sem:0.2f},  {top3_frac=:.2f}\\pm{top3_frac_sem:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 114\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# evaluate train\u001b[39;00m\n\u001b[1;32m    113\u001b[0m output \u001b[38;5;241m=\u001b[39m model(embs_qa)\n\u001b[0;32m--> 114\u001b[0m mrr, mtop1 \u001b[38;5;241m=\u001b[39m evaluate_retrieval(\n\u001b[1;32m    115\u001b[0m     output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    116\u001b[0m     model_corpus(embs_gt)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy(),\n\u001b[1;32m    117\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels, corpus_ids\u001b[38;5;241m=\u001b[39mdset\u001b[38;5;241m.\u001b[39mcorpus_ids)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mQA Train: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmrr\u001b[38;5;132;01m=:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmtop1\u001b[38;5;132;01m=:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# evaluate test\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "class LinearMapping(nn.Module):\n",
    "    def __init__(self, n_features, tfidf_size=None):\n",
    "        super(LinearMapping, self).__init__()\n",
    "        if tfidf_size is not None:\n",
    "            n_features = n_features - tfidf_size\n",
    "        self.linear = nn.Linear(n_features, n_features)\n",
    "        self.linear.weight.data = torch.eye(n_features, n_features)\n",
    "        self.tfidf_size = tfidf_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.tfidf_size is not None:\n",
    "            return torch.hstack((self.linear(x[:, :-self.tfidf_size]), x[:, -self.tfidf_size:]))\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class RescaleMapping(nn.Module):\n",
    "    def __init__(self, n_features, tfidf_size=None):\n",
    "        super(RescaleMapping, self).__init__()\n",
    "        if tfidf_size is not None:\n",
    "            n_features = n_features - tfidf_size\n",
    "\n",
    "        # elementwise multiply input by vector instead\n",
    "        self.linear = nn.Parameter(torch.ones(n_features))\n",
    "        self.linear.data = 1e-8 * self.linear.data\n",
    "        self.tfidf_size = tfidf_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # elementwise multiply input by vector instead\n",
    "        if self.tfidf_size is not None:\n",
    "            return torch.hstack((x[:, :-self.tfidf_size] * self.linear, x[:, -self.tfidf_size:]))\n",
    "        return x * self.linear\n",
    "\n",
    "\n",
    "# set args\n",
    "device = 'cuda'\n",
    "use_tfidf = True\n",
    "num_dissimilar_examples = 5\n",
    "# lr = 1e-1\n",
    "mapping = RescaleMapping\n",
    "# mapping = LinearMapping\n",
    "# note, need to consider what we are initializing linear models to (sometimes close to 0 is preferred, sometimes close to 1)\n",
    "# lr = 5e-3\n",
    "lr = 1e-4\n",
    "\n",
    "# get data\n",
    "embs_qa, embs_tfidf, labels = dset[query_ids_train]\n",
    "embs_qa_similar = np.vstack(\n",
    "    [dset.embs_qa_corpus_df.loc[lab[0]].values for lab in labels])\n",
    "embs_gt = dset.embs_qa_corpus_df.values\n",
    "embs_qa_test, embs_tfidf_test, labels_test = dset[query_ids_test]\n",
    "\n",
    "if use_tfidf:\n",
    "    embs_tfidf_similar = np.vstack(\n",
    "        [dset.embs_tfidf_corpus_df.loc[lab[0]].values for lab in labels])\n",
    "    embs_tfidf_gt = dset.embs_tfidf_corpus_df.values\n",
    "\n",
    "    # concatenate embs with tfidf\n",
    "    embs_qa = np.hstack([embs_qa, embs_tfidf])\n",
    "    embs_qa_similar = np.hstack([embs_qa_similar, embs_tfidf_similar])\n",
    "    embs_qa_test = np.hstack([embs_qa_test, embs_tfidf_test])\n",
    "    embs_gt = np.hstack([embs_gt, embs_tfidf_gt])\n",
    "    tfidf_size = embs_tfidf.shape[1]\n",
    "else:\n",
    "    tfidf_size = None\n",
    "\n",
    "\n",
    "# put all data on GPU\n",
    "def _tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "embs_qa = _tensor(embs_qa)\n",
    "embs_qa_similar = _tensor(embs_qa_similar)\n",
    "embs_qa_test = _tensor(embs_qa_test)\n",
    "embs_gt = _tensor(embs_gt)\n",
    "\n",
    "\n",
    "def get_dissimilar_examples(num_dissimilar_examples=25, use_tfidf=False):\n",
    "    embs_qa_dissimilar = np.vstack([\n",
    "        dset.embs_qa_corpus_df.loc[dset.get_random_neg_corpus_id(q)].values\n",
    "        for _ in range(num_dissimilar_examples)\n",
    "        for q in query_ids_train\n",
    "    ])\n",
    "    if use_tfidf:\n",
    "        embs_tfidf_dissimilar = np.vstack([\n",
    "            dset.embs_tfidf_corpus_df.loc[dset.get_random_neg_corpus_id(\n",
    "                q)].values\n",
    "            for _ in range(num_dissimilar_examples)\n",
    "            for q in query_ids_train\n",
    "        ])\n",
    "        embs_qa_dissimilar = np.hstack(\n",
    "            [embs_qa_dissimilar, embs_tfidf_dissimilar])\n",
    "    return _tensor(embs_qa_dissimilar)\n",
    "\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "model = mapping(\n",
    "    embs_qa.shape[1],\n",
    "    tfidf_size=tfidf_size).to('cuda')\n",
    "model_corpus = mapping(\n",
    "    embs_qa.shape[1],\n",
    "    tfidf_size=tfidf_size).to('cuda')\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.parameters()) + list(model_corpus.parameters()), lr=lr)\n",
    "for epoch in range(100):\n",
    "    model.eval()\n",
    "    # evaluate train\n",
    "    output = model(embs_qa)\n",
    "    mrr, mtop1 = evaluate_retrieval(\n",
    "        output.cpu().detach().numpy(),\n",
    "        model_corpus(embs_gt).cpu().detach().numpy(),\n",
    "        labels=labels, corpus_ids=dset.corpus_ids)\n",
    "    print(f'\\tQA Train: {mrr=:.3f}, {mtop1=:.3f}')\n",
    "\n",
    "    # evaluate test\n",
    "    output = model(embs_qa_test)\n",
    "    mrr, mtop1 = evaluate_retrieval(\n",
    "        output.cpu().detach().numpy(),\n",
    "        model_corpus(embs_gt).cpu().detach().numpy(),\n",
    "        labels=labels_test, corpus_ids=dset.corpus_ids)\n",
    "    print(f'\\tQA Test: {mrr=:.3f}, {mtop1=:.3f}')\n",
    "\n",
    "    # sample batch of dissimilar examples\n",
    "    embs_qa_dissimilar = get_dissimilar_examples(\n",
    "        num_dissimilar_examples, use_tfidf)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(embs_qa)\n",
    "    loss = criterion(output, model_corpus(embs_qa_similar)) - 0.1 * \\\n",
    "        criterion(torch.vstack(\n",
    "            [output] * num_dissimilar_examples), model_corpus(embs_qa_dissimilar))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch} Loss {loss.item():0.3e}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
