{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "import pandas as pd\n",
    "import os.path\n",
    "from os.path import join, expanduser\n",
    "from torch.utils.data import Dataset\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.optim\n",
    "from fit_mteb_pytorch import *\n",
    "\n",
    "# Load embeddings for corpus and queries\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "dset = MiniMarcoDataset()\n",
    "query_ids_train, query_ids_test = train_test_split(\n",
    "    dset.query_ids, random_state=1, test_size=0.2)\n",
    "\n",
    "# evaluate base embeddings\n",
    "embs_qa, embs_tfidf, labels = dset[query_ids_test]\n",
    "mrr, mtop1 = evaluate_retrieval(\n",
    "    embs_tfidf, dset.embs_tfidf_corpus_df.values,\n",
    "    labels=labels, corpus_ids=dset.corpus_ids)\n",
    "print(f'TF-IDF Test: {mrr=:.2f}, {mtop1=:.2f}')\n",
    "mrr, mtop1 = evaluate_retrieval(\n",
    "    embs_qa, dset.embs_qa_corpus_df.values,\n",
    "    labels=labels, corpus_ids=dset.corpus_ids)\n",
    "print(f'QA Test: {mrr=:.2f}, {mtop1=:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearMapping(nn.Module):\n",
    "    def __init__(self, n_features, tfidf_size=None):\n",
    "        super(LinearMapping, self).__init__()\n",
    "        if tfidf_size is not None:\n",
    "            n_features = n_features - tfidf_size\n",
    "        self.linear = nn.Linear(n_features, n_features)\n",
    "        self.linear.weight.data = torch.eye(n_features, n_features)\n",
    "        self.tfidf_size = tfidf_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.tfidf_size is not None:\n",
    "            return torch.hstack((self.linear(x[:, :-self.tfidf_size]), x[:, -self.tfidf_size:]))\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "class RescaleMapping(nn.Module):\n",
    "    def __init__(self, n_features, tfidf_size=None):\n",
    "        super(RescaleMapping, self).__init__()\n",
    "        if tfidf_size is not None:\n",
    "            n_features = n_features - tfidf_size\n",
    "\n",
    "        # elementwise multiply input by vector instead\n",
    "        self.linear = nn.Parameter(torch.ones(n_features))\n",
    "        self.linear.data = 1e-8 * self.linear.data\n",
    "        self.tfidf_size = tfidf_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        # elementwise multiply input by vector instead\n",
    "        if self.tfidf_size is not None:\n",
    "            return torch.hstack((x[:, :-self.tfidf_size] * self.linear, x[:, -self.tfidf_size:]))\n",
    "        return x * self.linear\n",
    "\n",
    "\n",
    "# set args\n",
    "device = 'cuda'\n",
    "use_tfidf = True\n",
    "num_dissimilar_examples = 5\n",
    "lr = 1e-1\n",
    "# note, need to consider what we are initializing linear models to (sometimes close to 0 is preferred, sometimes close to 1)\n",
    "# lr = 5e-3\n",
    "# lr = 1e-4\n",
    "\n",
    "# get data\n",
    "embs_qa, embs_tfidf, labels = dset[query_ids_train]\n",
    "embs_qa_similar = np.vstack(\n",
    "    [dset.embs_qa_corpus_df.loc[lab[0]].values for lab in labels])\n",
    "embs_gt = dset.embs_qa_corpus_df.values\n",
    "embs_qa_test, embs_tfidf_test, labels_test = dset[query_ids_test]\n",
    "\n",
    "if use_tfidf:\n",
    "    embs_tfidf_similar = np.vstack(\n",
    "        [dset.embs_tfidf_corpus_df.loc[lab[0]].values for lab in labels])\n",
    "    embs_tfidf_gt = dset.embs_tfidf_corpus_df.values\n",
    "\n",
    "    # concatenate embs with tfidf\n",
    "    embs_qa = np.hstack([embs_qa, embs_tfidf])\n",
    "    embs_qa_similar = np.hstack([embs_qa_similar, embs_tfidf_similar])\n",
    "    embs_qa_test = np.hstack([embs_qa_test, embs_tfidf_test])\n",
    "    embs_gt = np.hstack([embs_gt, embs_tfidf_gt])\n",
    "    tfidf_size = embs_tfidf.shape[1]\n",
    "else:\n",
    "    tfidf_size = None\n",
    "\n",
    "\n",
    "# put all data on GPU\n",
    "def _tensor(x):\n",
    "    return torch.tensor(x, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "embs_qa = _tensor(embs_qa)\n",
    "embs_qa_similar = _tensor(embs_qa_similar)\n",
    "embs_qa_test = _tensor(embs_qa_test)\n",
    "embs_gt = _tensor(embs_gt)\n",
    "\n",
    "\n",
    "def get_dissimilar_examples(num_dissimilar_examples=25, use_tfidf=False):\n",
    "    embs_qa_dissimilar = np.vstack([\n",
    "        dset.embs_qa_corpus_df.loc[dset.get_random_neg_corpus_id(q)].values\n",
    "        for _ in range(num_dissimilar_examples)\n",
    "        for q in query_ids_train\n",
    "    ])\n",
    "    if use_tfidf:\n",
    "        embs_tfidf_dissimilar = np.vstack([\n",
    "            dset.embs_tfidf_corpus_df.loc[dset.get_random_neg_corpus_id(\n",
    "                q)].values\n",
    "            for _ in range(num_dissimilar_examples)\n",
    "            for q in query_ids_train\n",
    "        ])\n",
    "        embs_qa_dissimilar = np.hstack(\n",
    "            [embs_qa_dissimilar, embs_tfidf_dissimilar])\n",
    "    return _tensor(embs_qa_dissimilar)\n",
    "\n",
    "\n",
    "# set random seed\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "random.seed(1)\n",
    "model = RescaleMapping(\n",
    "    embs_qa.shape[1],\n",
    "    tfidf_size=tfidf_size).to('cuda')\n",
    "model_corpus = RescaleMapping(\n",
    "    embs_qa.shape[1],\n",
    "    tfidf_size=tfidf_size).to('cuda')\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CosineEmbeddingLoss()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    list(model.parameters()) + list(model_corpus.parameters()), lr=lr)\n",
    "for epoch in range(100):\n",
    "    model.eval()\n",
    "    # evaluate train\n",
    "    output = model(embs_qa)\n",
    "    mrr, mtop1 = evaluate_retrieval(\n",
    "        output.cpu().detach().numpy(),\n",
    "        model_corpus(embs_gt).cpu().detach().numpy(),\n",
    "        labels=labels, corpus_ids=dset.corpus_ids)\n",
    "    print(f'\\tQA Train: {mrr=:.3f}, {mtop1=:.3f}')\n",
    "\n",
    "    # evaluate test\n",
    "    output = model(embs_qa_test)\n",
    "    mrr, mtop1 = evaluate_retrieval(\n",
    "        output.cpu().detach().numpy(),\n",
    "        model_corpus(embs_gt).cpu().detach().numpy(),\n",
    "        labels=labels_test, corpus_ids=dset.corpus_ids)\n",
    "    print(f'\\tQA Test: {mrr=:.3f}, {mtop1=:.3f}')\n",
    "\n",
    "    # sample batch of dissimilar examples\n",
    "    embs_qa_dissimilar = get_dissimilar_examples(\n",
    "        num_dissimilar_examples, use_tfidf)\n",
    "\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(embs_qa)\n",
    "    loss = criterion(output, model_corpus(embs_qa_similar)) - 0.1 * \\\n",
    "        criterion(torch.vstack(\n",
    "            [output] * num_dissimilar_examples), model_corpus(embs_qa_dissimilar))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f'Epoch {epoch} Loss {loss.item():0.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
